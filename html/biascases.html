<!-- HEAD-->

<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Female Perspective</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>

<!--HEADER-->
<header>

    <!-- h1 Female Perspective -->



    <h1 class="logo">
        <a href="index.html">
            <img src="../photos/fp-logo-blaa.png" alt="Female Perspective logo" class="logo-img">
            <div class="logo-text">
                Female<br>
                Perspective IT
            </div>
        </a>
    </h1>
    <nav>
        <ul>
            <li><a href="info.html" >Info</a></li> <!--note* active gør så den bliver bold når man holder oven den-->
            <li><a href="biascases.html" class="active">Bias Cases</a></li>
            <li><a href="timeline.html"> Rollemodeller </a></li>
            <li><a href="educations.html" >Udannelsesmuligheder</a></li>
            <li><a href="messageboard.html">Opslagstavle</a></li>
        </ul>
    </nav>
</header>

<!-- 6 BIAS CASES -->

<h2 class="biascasesh2"> BIAS CASES </h2>

<!-- INTRO -->

<section class="biascaseintro">
    <section class="biascaseintro1">
        <div class="why-are-women-necesary">
            <h3>Hvorfor er kvinder i IT afgørende?</h3>
            <br>
            Hvorfor teknologi skal udvikles af alle — ikke kun nogle få
            <br>
            I dag påvirker IT vores liv mere end nogensinde før. Vi bruger ansigtsgenkendelse til at låse vores telefoner op,
            AI til at vurdere jobansøgninger,
            <br>
            sundhedsapps til at overvåge kroppen og sikkerhedssystemer til at beskytte os i trafikken.
            <br>
            Men når teknologien bliver udviklet af teams, hvor én type bruger dominerer (typisk unge mænd), opstår der et problem:
            <br>
            Forskning fra MIT, Stanford, NIST og sundhedsorganisationer viser, at mange IT-systemer har markant højere fejlrate
            for kvinder — ikke fordi kvinder er “sværere” brugere,
            <br>
            men fordi modellerne og produkterne ikke er designet med dem i tankerne.
            <br>
        </div>
    </section>
    <br>

    <!-- når kvinder mangler i branchen -->
    <section class="women-biascase">
        <div class="when-women-are-needed">
            <h3>Når kvinder mangler i IT-branchen: </h3>
            <br>
            - Datasæt bliver skæve
            <br>
            - Designet tager udgangspunkt i mandekroppen
            <br>
            - Algoritmer overser kvinders behov
            <br>
            - Systemer fungerer dårligere for halvdelen af verdens befolkning
            <br>
        </div>
        <br>


        <!-- når kvinder deltager i udviklingen -->

        <div class="when-women-participate">
            <h3> Når kvinder deltager i udviklingen:</h3>
            <br>
            - Teknologi bliver mere retfærdig
            <br>
            - AI bliver mere præcis
            <br>
            - Produkter bliver mere brugervenlige
            <br>
            - Innovation bliver stærkere og mere kreativ
        </div>
    </section>
</section>




<section class="allbiascases">

    <section class="biascase1">

        <div class="biascase1">

            <div class="biascase1-photo">
                <img src="../photos/biascase1.png" alt="ansigtsgenkendelse kvinder">
            </div>

            <h3>
                Ansigtsgenkendelse fejler oftere for kvinder og personer med mørk hud
            </h3>
            <p>
                Ifølge NISTIR 8280, som har testet næsten 200 ansigtsgenkendelses‑algoritmer, er der tydelige forskelle i, hvor præcist systemerne genkender folk afhængigt af køn, alder og hudfarve.
                Kvinder — især med mørk hud — rammes oftere af fejl, såsom “false positives”, hvor systemet fejlagtigt siger, at to forskellige personer er den samme.
                Selv algoritmer med lave generelle fejlrater viser stadig store forskelle mellem demografiske grupper.
                Det betyder, at ansigtsgenkendelse i politiarbejde, kameraovervågning eller adgangskontrol kan give falske anklager eller diskrimination mod kvinder og personer med mørk hud.
                Teknologien er ikke neutral: Fejlene skyldes skæve datasæt og træning, ikke brugernes handlinger.
                For unge kvinder kan det ramme selvom man gør alt “korrekt” — man bliver udsat for systemets skævhed.
                Det viser, at ansvarlig brug, gennemsigtighed og fairness er afgørende, når man anvender ansigtsgenkendelse i samfundet.
                Konsekvens: Hyppigere fejl for kvinder og mørkhudede kan føre til falske anklager, urimelig mistænkeliggørelse og diskriminatio
            </p>
            <br>
            <p class="biascase-sources">
                Buolamwini & Gebru (MIT Media Lab, 2018) – Gender Shades
                PDF-rapport:
                <a href="https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf"> link </a>
                <br>

                NIST (National Institute of Standards and Technology, 2019)
                <a href="https://www.nist.gov/publications/frvt-part-3-demographic-effects"> link </a>
                <br>

                Fandt 10–100 gange højere falske match-rater for kvinder og mørkhudede ansigter
                Officiel rapport:
                <a href="https://www.nist.gov/publications/frvt-part-3-demographic-effects"> link </a>

            </p>
        </div>
    </section>


    <section class="biascase2" >

        <div class="biascasepicture">
            <div class="">
                <img src="" alt="">
            </div>
        </div>
        <h3>
            Sundhedsdata: Kvinder får dårligere hjælp, fordi standarden er mandekroppen
        </h3>
        <p>Sundhedsapps og retningslinjer for hjerteanfald bygger ofte på symptomer hos mænd.
            Ifølge British Heart Foundation og flere medicinske studier bliver kvinder 50 % oftere fejldiagnosticeret, når de får et hjerteanfald.
            Kvinder oplever oftere atypiske symptomer — som kvalme, åndenød eller smerter i ryg, hals eller kæbe — og 30 % af kvinder får slet ikke brystsmerter, som regnes for det “typiske mandlige symptom”.
            Det betyder, at både apps, akutvurderinger og kliniske beslutninger kan overse alvorlige faresignaler hos kvinder.
            Studier viser også, at kvinder i gennemsnit bliver 20–30 minutter senere behandlet i akutte situationer, fordi deres symptomer ikke passer til standardmodellen.
            Det er ikke kroppen, der er “forkert” — det er standarden, der er skæv.
            For unge kvinder betyder det, at man kan blive undervurderet eller misforstået i en medicinsk situation, selv når man har alvorlige symptomer.
            Det viser, hvor vigtigt det er, at sundhedsteknologi designes ud fra begge køn og ikke kun den mandlige model.
            Konsekvens: Kvinder får oftere senere — og dårligere — behandling, hvilket kan føre til farlige fejldiagnoser og unødig risiko.
        </p>
        <br>
        <p class="biascase-sources">
            British Heart Foundation (2019):
            <a href="        https://www.bhf.org.uk/informationsupport/heart-matters-magazine/medical/women-and-heart-attacks"> link </a>
            <br>

            American Heart Association – Heart Attack Symptoms in Women:
            <a href="        https://www.heart.org/en/health-topics/heart-attack/warning-signs-of-a-heart-attack"> link </a>
            <br>

            BMJ Study (2019): Kvinder har større risiko for forsinket eller forkert diagnose
            <a href="   https://www.bmj.com/content/363/bmj.k4247"> link </a>

        </p>
    </section>

    <section class="biascase3">

        <div class="biascase3">

            <div class="biascasepicture">
                <img src="" alt="">
            </div>
        </div>
        <h3>
            Recruitment-AI: Kvinder nedprioriteres systematisk
        </h3>

        <p>
            En AI-model brugt af et stort techfirma sorterede kvinder fra til IT-stillinger.
            Ifølge interne tests lærte systemet at nedprioritere CV’er, der indeholdt ord som “women’s”, fordi træningsdata primært kom fra mandlige ansøgere.
            Resultatet var, at algoritmen kopierede gamle kønsstereotyper og favoriserede mænd — også selvom kvinder havde samme eller bedre kvalifikationer.
            Selv når udviklerne forsøgte at fjerne enkelte ord, fortsatte modellen med at finde indirekte mønstre, der pegede på køn.
            Det viser, at selv højtudviklede AI-systemer kan reproducere diskrimination, hvis datasættet i forvejen er skævt.
            For kvinder betyder det, at man kan blive sorteret fra længe før et menneske ser ens ansøgning.
            Det understreger behovet for gennemsigtighed, fair træning og tydelige krav om ligebehandling i automatiseret rekruttering.
            Konsekvens: Automatiseret udvælgelse kan forstærke gamle kønsstereotyper og skabe ulige adgang til jobmuligheder.
        </p>
        <br>
        <p class="biascase-sources">
            Reuters (2018) – eksklusiv afsløring
            <a href="        https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G"> link </a>
        </p>
    </section>


    <section class="biascase4">

        <div class="biascase4">
            <div class="biascasepicture">
                <img src="" alt="">

            </div>
        </div>
        <h3>
            Hardware-design: Kvinders hænder passer dårligere til teknologien
        </h3>
        <p>Mange smartphones og wearables er designet ud fra gennemsnitlige mandlige håndmål.
            Ifølge antropometriske data ligger mænds gennemsnitlige håndlængde omkring 19,3 cm, mens kvinders ligger tættere på 17,2 cm.
            Alligevel overstiger flere moderne telefoner den størrelse, hvor én-hånds-brug er ergonomisk for de fleste kvinder.
            Det betyder kortere tommel-rækkevidde, dårligere greb og øget risiko for belastning ved almindelig brug.
            Ergonomiske studier viser, at kvinders hænder oftere bliver tvunget til yderpositioner, når de bruger store smartphones.
            Det er ikke tilfældigt — mange hardware-designs tager udgangspunkt i mandlige standardmål, mens kvinders behov sjældent indgår tidligt i udviklingen.
            For unge kvinder kan det gøre helt almindelige digitale aktiviteter mere besværlige, selvom man gør alting “rigtigt”.
            Det understreger, at inklusivt design kræver, at flere kropstyper bliver taget alvorligt i teknologisk produktudvikling.
            Konsekvens: For store enheder giver dårlig ergonomi og ulig adgang til komfortabel digital brug for mange kvinder.
        </p>
        <br>
        <p class="biascase-sources">
            NIOSH Anthropometric Data (CDC)
            <a href="        https://www.cdc.gov/niosh/data/datasets/rd-1006-2022-0/default.html"> link </a>
            <br>
            Cambridge University – ergonomi-studier
            <a href="        https://www.repository.cam.ac.uk/handle/1810/252736"> link </a>

        </p>
    </section>

    <section class="biascase5">

        <div class="biascase5">
            <div class="biascasepicture">
                <img src="" alt="">

            </div>
        </div>
        <h3>
            Bil-sikkerhedssystemer: Kvinder udsættes for større risiko
        </h3>
        <p>
            Moderne bilsikkerheds-IT — fra airbags til sensorer og kollisionsberegninger — fungerer dårligere for kvinder.
            Forskning fra University of Virginia viser, at kvinder har 73% større risiko for alvorlige skader i bilulykker, selv i identiske sammenstød.
            Derudover er risikoen for at dø 17% højere for kvinder sammenlignet med mænd.
            En central årsag er, at crash-test-dummies i årtier har været modelleret efter en mandlig standardkrop.
            Det betyder, at algoritmer, sensorer og sikkerhedsberegninger kalibreres efter mandlige proportioner, vægtfordeling og siddeposition.
            Når bilsikkerhed testes med skæve modeller, bliver kvinders anatomi bogstaveligt talt “usynlig” i udviklingen.
            For unge kvinder betyder det, at selv biler med høj sikkerhedsscore ikke nødvendigvis beskytter dem lige så effektivt.
            Teknologien virker altså ikke dårligere på grund af brugeren, men fordi designet ikke tager højde for kvinders kroppe.
            Det viser, at inklusiv sikkerhed kræver bedre modeller, flere datatyper og mere retfærdig testning.
            Konsekvens: Kvinder har større risiko for skader og dødsfald, fordi sikkerhedssystemerne er optimeret til mænd.
        </p>
        <br>
        <p class="biascase-sources">
            University of Virginia (2019)
            <a href="            https://news.virginia.edu/content/women-more-likely-men-suffer-injuries-car-crashes"> link </a>
            <br>
            NHTSA (US)
            <a href="https://www.nhtsa.gov"> link </a>
        </p>
    </section>

    <section class="biascase6">

        <div class="biascase6">
            <div class="biascasepicture">
                <img src="" alt="">

            </div>
        </div>
        <h3>
            Kamerasoftware: “Beauty mode” ændrer kvinders ansigter mere end mænds
        </h3>
        <p>
            Mange smartphones og fotoapps bruger automatisk “beauty mode”, som ændrer ansigter uden at man beder om det — og ændringerne rammer kvinder langt hårdere.
            Ifølge analyser fra bl.a. Washington Post og MIT Technology Review lysner algoritmer ofte hud, gør næsen smallere, øjnene større og huden glattere — især når brugeren er kvinde.
            Undersøgelser viser, at beauty-filtre justerer kvinders ansigter markant mere end mænds og ofte fremmer eurocentriske skønhedsidealer.
            I flere apps gælder beauty-mode endda kun for kvindelige brugere, mens mænd fotograferes mere “neutralt”.
            Resultatet er, at unge kvinder kan komme til at se et “forbedret” ansigt oftere end deres eget, hvilket påvirker selvbillede og selvtillid.
            Når kameraet kalibreres til at ændre kvinders ansigter mere end mænds, bliver skønhedsbegrebet ikke kun kulturelt — men teknologisk — biased.
            Det er ikke et spørgsmål om forfængelighed, men om algoritmer, der former, hvad der opfattes som “normalt”.
            For unge kvinder betyder det øget pres og urealistiske forventninger, som kan påvirke mental sundhed.
            Teknologien er ikke neutral, og brugere bør kunne vælge billeder uden skjulte forskønnelsesfiltre.
            Konsekvens: Skjulte beauty-filtre skaber urealistiske standarder og øger psykisk pres på især unge kvinder og piger.

        </p>
        <br>
        <p class="biascase-sources">
            Washington Post (2023): Smartphone beauty filters reinforce Eurocentric beauty standards
            <a href="        https://www.washingtonpost.com/technology/2023/01/11/beauty-filters-phone-cameras/"> link </a>
            <br>
            MIT Technology Review: Camera AI adds bias to facial smoothing
            <a href="        https://www.technologyreview.com/2021/06/10/1026004/beauty-ai-racism-sexism/"> link </a>
            <br>
            The Guardian: Beauty filters harm young women psychologically
            <a href="https://www.theguardian.com/technology/2021/mar/04/beauty-filters-young-women-self-esteem-tiktok"> link </a>

        </p>
    </section>

</section>

<section class="biascase7">

    <div class="biascase7">
        <div class="biascasepicture">
            <img src="" alt="">

        </div>
    </div>
    <h3>
        Algoritmer på sociale medier: Kvinders synlighed falder markant
    </h3>
    <p>Flere LinkedIn-eksperimenter viser, at algoritmer behandler kvindelige og mandlige profiler forskelligt.
        I SwitchTheSignal-eksperimentet skiftede kvinder deres LinkedIn-køn til “male” — og deres rækkevidde steg med +200–400%.
        Når mænd ændrede deres profil til “female”, faldt deres synlighed tilsvarende.
        Én enkelt ændring — køn — gav dramatisk forskel i, hvor meget ens indhold blev vist.
        Det viser, at kvinders synlighed på digitale platforme stadig er skrøbelig, og at algoritmer kan forstærke gamle kønsbias uden at nogen bemærker det.
        Hvis synligheden ændres så hurtigt i et offentligt netværk som LinkedIn, hvad sker der så i de algoritmer, vi ikke kan se — f.eks. i rekruttering, nyhedsfeeds eller anbefalingssystemer?
        For unge kvinder betyder det, at ens muligheder online ikke kun afhænger af, hvor aktiv eller dygtig man er — men også af algoritmer, der automatisk favoriserer mænd.
        Derfor er gennemsigtighed, fairness og kvinders deltagelse i AI-udvikling afgørende for fremtiden.
        Konsekvens: Når algoritmer nedprioriterer kvinder, skaber det lavere synlighed, færre muligheder og en digital verden, hvor kvinders stemmer bliver mindre hørt.
    </p>
    <br>
    <p class="biascase-sources">
        Linked in post fra Louise Sparf:
        <a href="https://www.linkedin.com/posts/louise-sparf_switchthesignal-genderbias-algorithmicbias-activity-7400856854634840064-HYYJ?utm_source=share&utm_medium=member_ios&rcm=ACoAAABWnHUBOyM04byCQwy-Tbtny_OsNx7k_ls"> link </a>
        <br>
        Linked in post fra Kathrine Bach:
        <a href="https://www.linkedin.com/posts/katrinebach_youve-probably-already-seen-the-viral-linkedin-activity-7401240883813228544-NUzv?utm_source=share&utm_medium=member_desktop&rcm=ACoAAFepNkQBOEi6lwv5v1SHDm_i5EAKyaEKDAw"> link </a>
        <br>
        Linked in post fra Megan Cornish:
        <a href="https://www.linkedin.com/posts/megan-cornish_i-changed-my-gender-to-male-on-linkedin-activity-7394735893917364224-GWq1?utm_source=share&utm_medium=member_desktop&rcm=ACoAAFepNkQBOEi6lwv5v1SHDm_i5EAKyaEKDAw"> link </a>


    </p>
</section>
</section>







</body>
<!-- FOOTER -->
<footer>
    <hr>
    <footer class="footer">

        <!-- Nyhedsbrev -->
        <div class="newsletter-section">
            <div class="newsletter-text">
                <h2>Nyhedsbrev</h2>
                <strong>Få information om Female Perspectives vigtige arbejde for unge kvinder direkte i din indbakke.</strong>
                <p>
                    Female Perspective kan kontakte dig med information om organisationens arbejde
                    via e-mail, telefonisk og via sociale medier. Hvis du ikke ønsker at vi kontakter
                    dig, kan du til enhver tid kontakte os på femaleperspective@info.dk. Vi lover at passe godt på din data. Du kan
                    læse vores <a href="#">privatlivspolitik her.</a>
                </p>
            </div>

            <form class="newsletter-form">
                <input type="email" placeholder="Din email *">
                <input type="text" placeholder="Dit fornavn">
                <button type="submit">Tilmeld dig</button>
            </form>
        </div>

        <hr>

        <!-- Footer links -->

        <div class="footer-columns">

            <div class="footer-col">
                <h3>Om Female Perspective</h3>
                <a href="#">Presse</a>
                <a href="#">Vores tilbud til skoler og gymnasier</a>
                <a href="#">Bliv frivillig</a>
                <a href="#">Ledige stillinger</a>
                <a href="#">Cookie- og privatlivspolitik</a>
                <a href="#">Betingelser</a>
            </div>

            <div class="footer-col">
                <h3>Følg os her</h3>
                <a href="#">Youtube</a>
                <a href="#">Instagram</a>
                <a href="#">Facebook</a>
                <a href="#">LinkedIn</a>
                <a href="#">TikTok</a>
                <a href="#">Spotify</a>
            </div>

            <div class="footer-col logo-col">
                <img src="../photos/fp-logo-blaa.png" alt="Female Perspective logo" class="footer-logo">
                <p>Meinungsgade 8<br>2200, Nørrebro</p>
                <p>CVR 01 23 45 67</p>
            </div>

            <div class="footer-col">
                <h3>Kontakt os</h3>
                <a href="mailto:femaleperspective@info.dk">femaleperspective@info.dk</a>
                <a href="tel:+45 12 34 56 78>">+45 12 34 56 78</a>
                <p>Telefontid mandag til fredag kl.<br>09.00 til 15.00</p>

                <a href="#">Female Perspectives FAQ Til Projektopgaver</a>
            </div>

        </div>

    </footer>
</html>